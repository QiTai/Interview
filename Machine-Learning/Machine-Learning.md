### NOTE

+ 当前互联网公司广告、搜索、推荐等都常用的 **LR、FM(Factorization Machine)、GBDT** 等，把原理搞透彻，例如LR的各种优化算法，FM的优化算法，传统GBDT和XGBOOST的原理区别等
+ [机器学习防止过拟合的方法](https://www.zhihu.com/question/59201590)
+ [GBDT与AdaBoost区别](https://www.zhihu.com/question/54626685)
+ [GBDT与XGboost区别](https://www.zhihu.com/question/54626685)
+ [集成模型GDBT与RandomForest效果好的原因](https://www.zhihu.com/question/51818176)
+ [SVD vs. PCA](https://www.zhihu.com/question/38319536)
  + SVD是PCA的另一种algebraic formulation,这也提供了另外一种算法来计算PCA
+ 为什么逻辑回归的优化目标是最大似然函数而不是均方差？
  + 个人总结：问题是一个二分类问题，y满足伯努利分布，而不是正太分布，可以参考Wikipedia-loss function for classification!
  + 参考[知乎帖](https://www.zhihu.com/question/24900876)
+ 数据不平衡怎么处理？
  + 基本思路：过采样，欠采样
+ k-means的目标函数->参考ML Techniques Taipei slides

+ 逻辑回归的极大似然估计的推导(to do)
+ SVM
  + SVM在什么情况下效果会比较差？(?)
  + SVM对偶问题(?)
+ PCA在什么情况下可以使用？(?)
+ [机器学习自测题](http://nooverfit.com/wp/12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%83%BD%E5%8A%9B%E8%87%AA%E6%B5%8B%E9%A2%98-%E7%9C%8B%E7%9C%8B%E4%BD%A0%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E8%83%BD%E6%89%93/) (to do?)
+ 怎么在online training下模拟bootstrap的过程(仔细读过Breiman的论文都会知道）(?)
+ MCMC(?)

+ 关于**时间序列**的卡尔曼滤波算法，HMM算法，波兹曼机等。——其中HMM适合做语音。?
+ **推荐系统**中协同过滤的各种模型。还有近邻搜索，LSH（Locality-Sensitive Hashing）等。?
